{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze with Q-Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matjoerg/my_python3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "import tensorflow as tf\n",
    "\n",
    "left = 0; right = 1; up = 2; down = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maze(position,action):\n",
    "    dead = False\n",
    "    actions = {left: [0,-1], right: [0,1], up: [1,0], down: [-1,0]}\n",
    "    state_matrix = np.array([[1, 1, 1, 1, 1, 1, 1],\n",
    "                             [1, 0, 0, 0, 0, 0, 1],\n",
    "                             [1, 0, 0, 0, 0, 0, 1],\n",
    "                             [1, 0, 0, 0, 0, 0, 1],\n",
    "                             [1, 0, 0, 0, 0, 0 ,1],\n",
    "                             [1, 0, 0, 1, 0, 2, 1],\n",
    "                             [1, 1, 1, 1, 1, 1, 1]])\n",
    "    \n",
    "    rewards = [-1, -20, 50]\n",
    "    \n",
    "    new_position = position + np.array(actions[action[0]])\n",
    "    new_state = state_matrix[int(new_position[0]),int(new_position[1])]\n",
    "    reward = rewards[new_state]\n",
    "    win = (new_state == 2)\n",
    "    if new_state == 1 or new_state == 2:\n",
    "        dead = True\n",
    "    \n",
    "    return reward, new_position, dead, win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct network feed-forward graph\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "W1 = tf.Variable(tf.random_uniform([2,100],0,0.01))\n",
    "b1 = tf.Variable(tf.zeros(shape=[1,100]))\n",
    "W2 = tf.Variable(tf.random_uniform([100,100],0,0.01))\n",
    "b2 = tf.Variable(tf.zeros(shape=[1,100]))\n",
    "W3 = tf.Variable(tf.random_uniform([100,4],0,0.01))\n",
    "b3 = tf.Variable(tf.zeros(shape=[1,4]))\n",
    "\n",
    "A1 = tf.nn.relu(tf.add(tf.matmul(inputs,W1),b1))\n",
    "A2 = tf.nn.relu(tf.add(tf.matmul(A1,W2),b2))\n",
    "Qout = tf.add(tf.matmul(A2,W3),b3)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "# model optimization\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, win rate: 0.5900\n",
      "Episode 200, win rate: 0.7550\n",
      "Episode 300, win rate: 0.7900\n",
      "Episode 400, win rate: 0.8125\n",
      "Episode 500, win rate: 0.8220\n",
      "Episode 600, win rate: 0.8350\n",
      "Episode 700, win rate: 0.8443\n",
      "Episode 800, win rate: 0.8512\n",
      "Episode 900, win rate: 0.8567\n",
      "Episode 1000, win rate: 0.8590\n",
      "Episode 1100, win rate: 0.8573\n",
      "Episode 1200, win rate: 0.8550\n",
      "Episode 1300, win rate: 0.8523\n",
      "Episode 1400, win rate: 0.8543\n",
      "Episode 1500, win rate: 0.8567\n",
      "Episode 1600, win rate: 0.8562\n",
      "Episode 1700, win rate: 0.8594\n",
      "Episode 1800, win rate: 0.8639\n",
      "Episode 1900, win rate: 0.8674\n",
      "Episode 2000, win rate: 0.8695\n",
      "Episode 2100, win rate: 0.8710\n",
      "Episode 2200, win rate: 0.8714\n",
      "Episode 2300, win rate: 0.8717\n",
      "Episode 2400, win rate: 0.8721\n",
      "Episode 2500, win rate: 0.8744\n",
      "Episode 2600, win rate: 0.8750\n",
      "Episode 2700, win rate: 0.8752\n",
      "Episode 2800, win rate: 0.8768\n",
      "Episode 2900, win rate: 0.8748\n",
      "Episode 3000, win rate: 0.8753\n",
      "Episode 3100, win rate: 0.8761\n",
      "Episode 3200, win rate: 0.8756\n",
      "Episode 3300, win rate: 0.8764\n",
      "Episode 3400, win rate: 0.8771\n",
      "Episode 3500, win rate: 0.8783\n",
      "Episode 3600, win rate: 0.8797\n",
      "Episode 3700, win rate: 0.8803\n",
      "Episode 3800, win rate: 0.8805\n",
      "Episode 3900, win rate: 0.8808\n",
      "Episode 4000, win rate: 0.8800\n",
      "Episode 4100, win rate: 0.8807\n",
      "Episode 4200, win rate: 0.8807\n",
      "Episode 4300, win rate: 0.8802\n",
      "Episode 4400, win rate: 0.8814\n",
      "Episode 4500, win rate: 0.8820\n",
      "Episode 4600, win rate: 0.8826\n",
      "Episode 4700, win rate: 0.8834\n",
      "Episode 4800, win rate: 0.8840\n",
      "Episode 4900, win rate: 0.8831\n",
      "Episode 5000, win rate: 0.8830\n",
      "Episode 5100, win rate: 0.8837\n",
      "Episode 5200, win rate: 0.8837\n",
      "Episode 5300, win rate: 0.8834\n",
      "Episode 5400, win rate: 0.8848\n",
      "Episode 5500, win rate: 0.8849\n",
      "Episode 5600, win rate: 0.8854\n",
      "Episode 5700, win rate: 0.8854\n",
      "Episode 5800, win rate: 0.8852\n",
      "Episode 5900, win rate: 0.8853\n",
      "Episode 6000, win rate: 0.8847\n",
      "Episode 6100, win rate: 0.8849\n",
      "Episode 6200, win rate: 0.8848\n",
      "Episode 6300, win rate: 0.8846\n"
     ]
    }
   ],
   "source": [
    "# learning parameters\n",
    "y = 0.99\n",
    "e = 0.05\n",
    "num_episodes = 10000\n",
    "\n",
    "# starting position\n",
    "p0 = [5.,1.]\n",
    "\n",
    "# save positions and rewards\n",
    "rList = []\n",
    "xList = []\n",
    "yList = []\n",
    "WList = []\n",
    "wins = 0\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        if (i+1)%100 == 0:\n",
    "            print(\"Episode {}, win rate: {:.4f}\".format(i+1,wins/float(i+1)))\n",
    "        p = p0\n",
    "        rAll = []\n",
    "        xAll = [p[1]]\n",
    "        yAll = [6-p[0]]\n",
    "        dead = False\n",
    "        step = 0\n",
    "        while step < 99:\n",
    "            step += 1\n",
    "            # choose action\n",
    "            a, allQ = sess.run([predict,Qout],feed_dict={inputs:np.array([p])})\n",
    "            #print(a, allQ)\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(0,4)\n",
    "            # new position and reward\n",
    "            r, p_new, dead, win = Maze(p,a)\n",
    "            wins += win\n",
    "            # estimate next Q' values with network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs:np.array([p_new])})\n",
    "            #print(p,p_new)\n",
    "            maxQ1 = np.max(Q1)\n",
    "            # calculate target Q\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            # train network\n",
    "            _,Wout = sess.run([updateModel,W3],feed_dict={inputs:np.array([p]),nextQ:targetQ})\n",
    "            p = p_new\n",
    "            rAll.append(r)\n",
    "            xAll.append(p[1])\n",
    "            yAll.append(6-p[0])\n",
    "            if dead is True:\n",
    "                break\n",
    "        rList.append(np.sum(rAll))\n",
    "        xList.append(xAll)\n",
    "        yList.append(yAll)\n",
    "        WList.append(np.sum(Wout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Win rate: {}\".format(wins/float(num_episodes)))\n",
    "plot(range(num_episodes),rList)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1,1,'C0o',ms=10)\n",
    "plot(5,1,'C0x',ms=10)\n",
    "plot(xList[-1],yList[-1],'C3-')\n",
    "plot([3,3],[0,3],'k-',lw=5)\n",
    "xlim([0,6])\n",
    "ylim([0,6])\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
